input {
  file {
    path => "D:/ELK/logstash-7.8.1/movies.csv"
    start_position => "beginning"
    sincedb_path => "D:/ELK/logstash-7.8.1/offset"
  }
  # redis, file, jdbc, kafka, syslog这几个输入源
  jdbc {
    jdbc_connection_string => "jdbc:mysql://127.0.0.1:3306/explain"
    jdbc_user => "root"
    jdbc_password => "root"
    jdbc_driver_library => "mysql-connector-java-5.1.36-bin.jar"
    jdbc_driver_class => "com.mysql.jdbc.Driver"
    schedule => "* * * * *" ## 定时任务规则 每分钟执行一次
    statement => "SELECT * from user where add_time > ? and name = ? " # :sql_last_value 为上次增量同步的时间
    prepared_statement_bind_values => [":sql_last_value", "二哥"]
    # 用于保存:sql_last_value值的文件,因为可能会定义多个:sql_last_value,系统会默认一个文件,可能出现相互覆盖的情况
    last_run_metadata_path => "/usr/temp"
    clean_run => "false" ## 是否每次清除 last_run_metadata_path文件
    statement_filepath => "" # sql语句文件
    use_column_value => true
    use_prepared_statements => true
    #是否开启记录追踪
    record_last_run => "true"
    #是否需要追踪字段，如果为true，则需要指定tracking_column，默认是timestamp
    use_column_value => "true"
    #指定追踪的字段
    tracking_column => "add_time"
    #追踪字段的类型，目前只有数字和时间类型，默认是数字类型
    tracking_column_type => "timestamp"
    columns_charset => { "name" => "ISO-8859-1" } ## 列字符集
  }
}
filter {
  # grok 解析并构造文本
  csv {
    separator => ","
    columns => ["id","content","genre"]
  }
  ## mutate类型过滤器
  mutate {
    split => { "genre" => "|" }
    remove_field => ["path", "host","@timestamp","message"] ## 移除字段
  }

  mutate {
    add_field => {"testField1" => "0"} ## 添加字段 并设置默认值???
    add_field => {"testField2" => "%{name}"} ##引用name中的值
  }

  mutate { ## 复制字段到另一个字段, 另一个字段不需要单独添加
    copy => {"name" => "name2"}
  }

  mutate { ## 正则表达式 将name字段中的o替换成p
    gsub => ["name","o","p"]
  }

  mutate {
    convert => { ## 将字段类型转为指定类型
      "name" => "string"
      "age" => "integer"
    }
  }

  mutate {
    # lowercase => [ "name" ] 将字段转为大写
    uppercase => [ "name" ]
  }

  mutate { ## 重命名字段
    rename => {"name" => "name3"}
  }

  mutate { ## 去除name字段的前后空格
    strip => ["name"]
  }

  mutate { ## 更新字段的值
    update => {"name" => "li"}
  }

  mutate { ## 更新字段的值
    replace => { "message" => "%{source_host}: My new message" }
  }
  mutate {
    split => ["content", "("] ## 切分
    add_field => { "title" => "%{[content][0]}"}
    add_field => { "year" => "%{[content][1]}"}
  }
  mutate {## 将数据类型为array的进行拼接 | 分隔
    join => ["message", "|"]
  }
  mutate {
    convert => {
      "year" => "integer"
    }
    strip => ["title"]
    remove_field => ["path", "host","@timestamp","message","content"]
  }

}
output {
  # elasticsearch, kafka, file 这三类输出方式
  elasticsearch {
    hosts => "http://localhost:9200"
    index => "movies"
    document_id => "%{id}"
  }
  stdout {}
}
# 执行顺序
#    rename(event) if @rename
#    update(event) if @update
#    replace(event) if @replace
#    convert(event) if @convert
#    gsub(event) if @gsub
#    uppercase(event) if @uppercase
#    lowercase(event) if @lowercase
#    strip(event) if @strip
#    remove(event) if @remove
#    split(event) if @split
#    join(event) if @join
#    merge(event) if @merge
#    filter_matched(event)